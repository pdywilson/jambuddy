{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1546,
     "status": "ok",
     "timestamp": 1559137401049,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "VSrdFgxyDrmG",
    "outputId": "c68ff6ce-79af-4ac4-d7f8-d4ccbbee7f80",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import collections\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "#import pyfluidsynth\n",
    "\n",
    "from pathlib import Path\n",
    "import pretty_midi\n",
    "import librosa.display\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "folder = 'train/'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1559137401055,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "YbO_nrQpDrmO",
    "outputId": "2a1e1eec-c5a2-47a0-b9ba-e92467767ef4"
   },
   "outputs": [],
   "source": [
    "# some prep steps\n",
    "# for google colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    \n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    data_folder = \"/content/drive/My Drive/Colab Notebooks/data/\"\n",
    "    os.chdir(data_folder)\n",
    "else:\n",
    "    data_folder = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95G80nAcDrmS"
   },
   "source": [
    "# A. Preprocess Midi file\n",
    "#### Choose a harmony and a melody track from a MIDI file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iSk5F55lDrmb"
   },
   "outputs": [],
   "source": [
    "## PARAMS\n",
    "# highest note is melody\n",
    "LOW = 48 # C2=48, C3=60, C4=72, C5=84, C6=96\n",
    "HIGH = 84 # range(48,84) = 36 notes \n",
    "\n",
    "# input sizes\n",
    "MELODY = 37 \n",
    "TIMES = 48\n",
    "CHORDS = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eYnJdXBYDrmf"
   },
   "outputs": [],
   "source": [
    "# MIDI processing\n",
    "def getMelodyAndHarmony(file):\n",
    "    def padAndAdd(a,b):\n",
    "        return sorted(a+b,key= lambda x:x.start)\n",
    "    \n",
    "    pm = pretty_midi.PrettyMIDI(file)\n",
    "    pm.remove_invalid_notes()\n",
    "    print(\"\\nEasy Processing: \"+file)\n",
    "    \n",
    "    if len(pm.instruments) == 1:\n",
    "        print(\"Both: Using only track for both harmony and melody\")\n",
    "        pianoroll = pm.instruments[0].notes\n",
    "        return {'melody': pianoroll, 'harmony': pianoroll}\n",
    "    \n",
    "    else:\n",
    "        maxlen = 0\n",
    "        pianos = []\n",
    "        guitars = []\n",
    "        melodies = []\n",
    "        for i, elt in enumerate(pm.instruments):\n",
    "            #if pm.instruments[i].is_drum:\n",
    "            #    continue\n",
    "            pianoroll = pm.instruments[i].get_piano_roll().T\n",
    "            numnotes = pianoroll.nonzero()[0].shape[0]\n",
    "            if numnotes > maxlen:\n",
    "                mostnotes = i\n",
    "                maxlen = numnotes\n",
    "            if 'piano' in pm.instruments[i].name.lower():             #if pm.instruments[i].program in range(8): # MIDI 0..7 are pianos\n",
    "                pianos.append((pm.instruments[i].notes,numnotes))\n",
    "            if 'guitar' in pm.instruments[i].name.lower():             #if pm.instruments[i].program in range(24,32):\n",
    "                guitars.append((pm.instruments[i].notes,numnotes))\n",
    "            if 'melody' in pm.instruments[i].name.lower():\n",
    "                # melodies.append((pianoroll,numnotes)) do i really want to padandadd two melodies always??\n",
    "                melodies.append(pm.instruments[i].notes)\n",
    "\n",
    "        if pianos != []:\n",
    "            print(\"Harmony: Using piano with most notes as harmony of\", len(pianos))\n",
    "            pianowithmostnotes = max(pianos, key=lambda x: x[1])\n",
    "            harmonytrack = pianowithmostnotes[0]\n",
    "        elif guitars != []:\n",
    "            print(\"Harmony: Using guitar with most notes as harmony of\", len(guitars))\n",
    "            guitarwithmostnotes = max(guitars, key=lambda x: x[1])\n",
    "            harmonytrack = guitarwithmostnotes[0]\n",
    "        else:\n",
    "            print(\"Error: no piano or guitar track found\")\n",
    "            harmonytrack = pm.instruments[0].notes\n",
    "            return\n",
    "\n",
    "        if len(melodies) == 1:\n",
    "            print(\"Melody: using only melody track as melody\")\n",
    "            melodytrack = melodies[0]\n",
    "        elif len(melodies) == 2:\n",
    "            print(\"Melody: padding and adding two melodies to one\")\n",
    "            melodytrack = padAndAdd(melodies[0], melodies[1])\n",
    "        elif len(melodies) > 2:\n",
    "            print(\"Melody: more than 2 melody tracks. padding and adding first two of # \", len(melodies),\"melodies.\")\n",
    "            melodytrack = padAndAdd(melodies[0], melodies[1])\n",
    "        else:\n",
    "            print(\"Error: no melody track found.\")\n",
    "            melodytrack = pm.instruments[0].notes\n",
    "            return\n",
    "\n",
    "        return {'melody': melodytrack, 'harmony': harmonytrack}\n",
    "\n",
    "def oneHot(idx, arraysize):\n",
    "        if idx >= arraysize:\n",
    "            print(\"error idx > arraysize\")\n",
    "            return\n",
    "        ar = np.zeros(arraysize)\n",
    "        ar[idx] = 1\n",
    "        return ar\n",
    "\n",
    "def processMidi(file):\n",
    "    pm = pretty_midi.PrettyMIDI(file)\n",
    "    tracks = getMelodyAndHarmony(file)\n",
    "\n",
    "    pmnotes = tracks['melody']\n",
    "    notes, times = [], []\n",
    "    currenttime = 0\n",
    "    for i, note in enumerate(pmnotes):\n",
    "        if note.start > currenttime:\n",
    "            notes.append(MELODY-1)\n",
    "            duration = pm.time_to_tick(note.start-currenttime)\n",
    "            times.append(duration)\n",
    "        if note.pitch in range(LOW,HIGH):\n",
    "            notes.append(note.pitch - LOW)\n",
    "        else:\n",
    "            notes.append(note.pitch % 12)\n",
    "        if i < len(pmnotes)-1 and note.end > pmnotes[i+1].start: # overlap with next note\n",
    "            times.append(pm.time_to_tick(pmnotes[i+1].start - note.start))\n",
    "            currenttime = pmnotes[i+1].start\n",
    "        else:\n",
    "            times.append(pm.time_to_tick(note.end-note.start))\n",
    "            currenttime = note.end\n",
    "    times = np.array(np.round(np.array(times)/pm.resolution*12),dtype=np.int32)\n",
    "    \n",
    "    # getChords\n",
    "    pmnotes = tracks['harmony']\n",
    "    notesDict = {}\n",
    "    for i, note in enumerate(pmnotes):\n",
    "        end = note.end\n",
    "        startbeat = int(pm.time_to_tick(note.start)/pm.resolution)\n",
    "        endbeat = int(pm.time_to_tick(note.end)/pm.resolution)\n",
    "        for j in range(startbeat, endbeat): #evry beat\n",
    "            notesDict[j] = notesDict.get(j,np.zeros(12)) + oneHot(note.pitch % 12,12)\n",
    "    \n",
    "\n",
    "    # append for every beat (timestep) a vector. either np.zeros or chordinfovector\n",
    "    chords = []\n",
    "    for i in range(max(notesDict)): \n",
    "        chords.append(notesDict.get(i,np.zeros(12)))\n",
    "    chordsByBeat = np.array(chords)\n",
    "    \n",
    "\n",
    "    # connect notes and chords\n",
    "    currbeat = 0\n",
    "    newchords = []\n",
    "    for time in times:\n",
    "        beat = int(currbeat/12)\n",
    "        newchord = np.zeros(12)\n",
    "        for j in range(int((time)/12)): \n",
    "            if j+beat < chordsByBeat.shape[0]:\n",
    "                newchord += chordsByBeat[j+beat]\n",
    "        newchords.append(newchord)\n",
    "        currbeat += time\n",
    "    chords = np.array(newchords)\n",
    "    \n",
    "    \n",
    "     #cleanup\n",
    "    times[np.where(times > 48)[0]] = 48\n",
    "    notes = np.array(notes)[np.where(times>0)[0]]\n",
    "    chords = chords[np.where(times > 0)[0]]\n",
    "    times = times[np.where(times > 0)[0]]\n",
    "    times = times - 1 # reshift to 0..47\n",
    "    \n",
    "    \n",
    "    return notes, times, chords, chordsByBeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kvKT9W1Drmi"
   },
   "outputs": [],
   "source": [
    "# Input representation\n",
    "def getCircleOfThirds(note):\n",
    "    \"\"\"input: note, scalar in [0,C)\n",
    "    output: array, size (11,) including 3 bits for octave, 1 for rest\n",
    "    \"\"\"\n",
    "    circleMajor = lambda x: x%4\n",
    "    circleMinor = lambda x: x%3\n",
    "    representation = np.zeros(11)\n",
    "    if note < MELODY-1:\n",
    "        absnote = note % 12\n",
    "        octave = int(note/12)\n",
    "        representation[7+octave] = 1\n",
    "        representation[circleMajor(absnote)] = 1\n",
    "        representation[4+circleMinor(absnote)] = 1\n",
    "    else:\n",
    "        representation[-1] = 1\n",
    "    return representation\n",
    "\n",
    "def getDuration(duration):\n",
    "    \"\"\"input size (1,) in [0,48)\n",
    "       output size (6,)\n",
    "    \"\"\"\n",
    "    return np.unpackbits(np.uint8(duration))[2:]\n",
    "\n",
    "def getChord(chord):\n",
    "    \"\"\"input size (12,)\n",
    "    outputs current and next chord shape (7,)\n",
    "    \"\"\"\n",
    "    repre = np.zeros(7)\n",
    "    for i, elt in enumerate(chord):\n",
    "        x = elt * getCircleOfThirds(i)[:7]\n",
    "        repre += x\n",
    "    return repre\n",
    "\n",
    "def getFeatureVectors(notes, times, chords, encodingDict, modulation=False): #TODO modulation ueberdenken\n",
    "    \"\"\"input size [7(pitch)+2(octaves)+1(Rest)]+6(duration)+7(chords)+7(nextchords)=30=Fsize #TODO: maybe change chords\n",
    "    Creates Feature Vector from notes lists.\n",
    "    Input: notes, times, chords from processMIDI\n",
    "    Output: features array, size: (N, Fsize)  Fsize varies\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for i, note in enumerate(notes[:-1]):\n",
    "        feature = np.empty(0)\n",
    "        if encodingDict['melody']:\n",
    "            if notes[i] < MELODY: # pitch or pause bit\n",
    "                feature = np.concatenate((feature, oneHot(notes[i],MELODY)))\n",
    "            else:\n",
    "                print(\"melody over 37\")\n",
    "                feature = np.concatenate((feature, np.zeros(MELODY)))\n",
    "            \n",
    "        if encodingDict['melodyModulo']: \n",
    "            if notes[i] < MELODY-1: # only pitch bit or zeros if pause\n",
    "                feature = np.concatenate((feature, oneHot(notes[i]%12,12)))\n",
    "            else:\n",
    "                feature = np.concatenate((feature, np.zeros(12)))\n",
    "        if encodingDict['melodyEncoded']: # pitch+octave+rest\n",
    "            feature = np.concatenate((feature,getCircleOfThirds(note)))\n",
    "        if encodingDict['duration']:\n",
    "            feature = np.concatenate((feature,oneHot(int(times[i]),48)))  \n",
    "        if encodingDict['durationEncoded']:\n",
    "            feature = np.concatenate((feature,getDuration(times[i])))\n",
    "        if encodingDict['chordsNormally']:\n",
    "            feature = np.concatenate((feature,chords[i]))\n",
    "            feature = np.concatenate((feature,chords[i+1]))\n",
    "        if encodingDict['chordsEncoded']:\n",
    "            feature = np.concatenate((feature,getChord(chords[i])))  # chord ERROR: MAKE THIS BETTER\n",
    "            feature = np.concatenate((feature,getChord(chords[i+1])))   # chord ERROR: MAKE THIS BETTER\n",
    "        features.append(feature)\n",
    "    features = np.array(features)\n",
    "    return features.astype('float32')\n",
    "\n",
    "def getInputSequences(notes, times, chords, encodingDict, modulation=False, padding=False, seq_len=16):\n",
    "    \"\"\"Create Net Input.\n",
    "    Input: features from getFeatureVectors or getEncodedFeatureVectors\n",
    "    Output: \n",
    "        - Sequences, size: (157, seq_len)\n",
    "        - Labels, one-hot, size: (MELODY,)\n",
    "    \"\"\"\n",
    "    features = getFeatureVectors(notes, times, chords, encodingDict, modulation)\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    if padding == True:\n",
    "        padded_notes = np.concatenate((np.zeros((seq_len,features.shape[1])),features[:seq_len]))\n",
    "        for j in range(seq_len):\n",
    "            sequences.append(padded_notes[j:j+seq_len])\n",
    "            label_pitch = notes[j]\n",
    "            label_duration = times[j]\n",
    "            labels.append((label_pitch, label_duration))\n",
    "    while i+seq_len < features.shape[0]:\n",
    "        sequences.append(features[i:i+seq_len])\n",
    "        label_pitch = notes[i+seq_len]\n",
    "        label_duration = times[i+seq_len]\n",
    "        labels.append((label_pitch, label_duration))\n",
    "        i += 1     # or += seq_len\n",
    "        \n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "    return sequences, labels\n",
    "\n",
    "def modulateOld(notes, times, chords, chordsByBeat, modulate):\n",
    "    newnotesli = np.empty(0)\n",
    "    newchords, newchordsbb = [], []\n",
    "    for i in range(12):\n",
    "        newnotes = notes - i\n",
    "        newnotes[np.where(newnotes == 36-i)[0]] = 36\n",
    "        newnotes[np.where(newnotes < 0)[0]] = 36\n",
    "        newnotesli = np.concatenate((newnotesli,newnotes))\n",
    "\n",
    "        newc = np.append(chords[:,i:],chords[:,:i],axis=1)\n",
    "        newchords.append(newc)\n",
    "\n",
    "        newcbb = np.append(chordsByBeat[:,i:],chordsByBeat[:,:i],axis=1)\n",
    "        newchordsbb.append(newcbb)\n",
    "\n",
    "    newchords = np.array(newchords).reshape(-1,12)\n",
    "    newchordsbb = np.array(newchordsbb).reshape(-1,12)\n",
    "    return newnotesli, times, newchords, newchordsbb\n",
    "\n",
    "def modulate(notes, chords, chordsByBeat, modulate=0):\n",
    "    \"\"\"modulates downwards\"\"\"\n",
    "    i = modulate\n",
    "    modnotes = notes - i\n",
    "    modnotes[np.where(modnotes == 36-i)[0]] = 36\n",
    "    modnotes[np.where(modnotes < 0)[0]] = 36\n",
    "\n",
    "    modchords = np.append(chords[:,i:],chords[:,:i],axis=1)\n",
    "\n",
    "    if chordsByBeat is not None:\n",
    "        modcbb = np.append(chordsByBeat[:,i:],chordsByBeat[:,:i],axis=1)\n",
    "    else:\n",
    "        modcbb = None\n",
    "\n",
    "    return modnotes, modchords, modcbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVE0og8fDrmk"
   },
   "outputs": [],
   "source": [
    "# Choose encoding and load files\n",
    "encodingDict = {\n",
    "    'melody': True,\n",
    "    'melodyModulo': True,\n",
    "    'melodyEncoded': False,\n",
    "    'duration': True,\n",
    "    'durationEncoded': False,\n",
    "    'chordsNormally': True,\n",
    "    'chordsEncoded': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jBnOuprDrmn"
   },
   "source": [
    "# B. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1559137409960,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "0SedqNjRDrmo",
    "outputId": "6c490066-93fe-45fb-c10e-b9e38cd766f6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trainprep.mid',\n",
       " 'acertainsmile.mid',\n",
       " 'affairinsanmiguel.mid',\n",
       " 'adayinalifeofafool.mid',\n",
       " 'girlfromipanemaprep.mid',\n",
       " 'aftertherainhasfallen.mid',\n",
       " 'autumnleaves.mid',\n",
       " 'browneyedgirlprep.mid',\n",
       " 'swayprep.mid',\n",
       " 'amazinggraceprep.mid',\n",
       " 'myfunnyvalentineprep.mid',\n",
       " 'newyorkprep.mid',\n",
       " 'afterlovehasgone.mid',\n",
       " 'dreamalittledramprep.mid',\n",
       " '634.mid']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gets all .mid filenames from folder train/ in one line\n",
    "files = [file for file in os.listdir(\"train\") if '.mid' in file]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1054
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31731,
     "status": "ok",
     "timestamp": 1559137441322,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "piJAllN6Drmr",
    "outputId": "8c1347b5-b909-47e8-8892-3157fb82f377",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Easy Processing: train/trainprep.mid\n",
      "Harmony: Using piano with most notes as harmony of 1\n",
      "Melody: using only melody track as melody\n",
      "\n",
      "Easy Processing: train/acertainsmile.mid\n",
      "Harmony: Using piano with most notes as harmony of 1\n",
      "Melody: using only melody track as melody\n",
      "\n",
      "Easy Processing: train/affairinsanmiguel.mid\n",
      "Harmony: Using piano with most notes as harmony of 1\n",
      "Melody: more than 2 melody tracks. padding and adding first two of #  6 melodies.\n",
      "\n",
      "Easy Processing: train/adayinalifeofafool.mid\n",
      "Harmony: Using guitar with most notes as harmony of 1\n",
      "Melody: padding and adding two melodies to one\n",
      "\n",
      "Easy Processing: train/girlfromipanemaprep.mid\n",
      "Harmony: Using piano with most notes as harmony of 1\n",
      "Melody: using only melody track as melody\n",
      "\n",
      "Easy Processing: train/aftertherainhasfallen.mid\n",
      "Harmony: Using piano with most notes as harmony of 4\n",
      "Melody: using only melody track as melody\n",
      "\n",
      "Easy Processing: train/autumnleaves.mid\n",
      "Harmony: Using piano with most notes as harmony of 1\n",
      "Melody: padding and adding two melodies to one\n",
      "\n",
      "Easy Processing: train/browneyedgirlprep.mid\n",
      "Harmony: Using guitar with most notes as harmony of 1\n",
      "Melody: using only melody track as melody\n",
      "\n",
      "Easy Processing: train/swayprep.mid\n",
      "Harmony: Using piano with most notes as harmony of 1\n",
      "Melody: using only melody track as melody\n",
      "\n",
      "Easy Processing: train/amazinggraceprep.mid\n",
      "Harmony: Using guitar with most notes as harmony of 1\n",
      "Melody: padding and adding two melodies to one\n",
      "\n",
      "Easy Processing: train/myfunnyvalentineprep.mid\n",
      "Harmony: Using piano with most notes as harmony of 1\n",
      "Melody: using only melody track as melody\n",
      "\n",
      "Easy Processing: train/newyorkprep.mid\n",
      "Harmony: Using guitar with most notes as harmony of 1\n",
      "Melody: more than 2 melody tracks. padding and adding first two of #  3 melodies.\n",
      "\n",
      "Easy Processing: train/afterlovehasgone.mid\n",
      "Harmony: Using piano with most notes as harmony of 1\n",
      "Melody: using only melody track as melody\n",
      "\n",
      "Easy Processing: train/dreamalittledramprep.mid\n",
      "Harmony: Using guitar with most notes as harmony of 1\n",
      "Melody: using only melody track as melody\n",
      "\n",
      "Easy Processing: train/634.mid\n",
      "Harmony: Using guitar with most notes as harmony of 1\n",
      "Melody: using only melody track as melody\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((71199, 16, 121), (71199, 2))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modulation = True\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    notes, times, chords, chordsByBeat = processMidi(folder+file)\n",
    "    if modulation == True:\n",
    "        for mod_i in range(12): # modulate\n",
    "            modnotes, modchords, _ = modulate(notes, chords, chordsByBeat, modulate = mod_i)\n",
    "            sequences, labels = getInputSequences(modnotes, times, modchords, encodingDict, padding=False, seq_len=16)\n",
    "            X = sequences if i==0 else np.concatenate((X, sequences))\n",
    "            y = labels if i==0 else np.concatenate((y, labels))\n",
    "    else:\n",
    "        sequences, labels = getInputSequences(notes, times, chords, encodingDict, padding=False, seq_len=16)\n",
    "        X = sequences if i==0 else np.concatenate((X, sequences))\n",
    "        y = labels if i==0 else np.concatenate((y, labels))\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31472,
     "status": "ok",
     "timestamp": 1559137442079,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "Uv4TY09ZDrmv",
    "outputId": "92017864-0a90-409f-b5c3-d61df27aee3c",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13d9bd7b8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stats of training data\n",
    "pitches = y[:,0]\n",
    "durations = y[:,1]\n",
    "sns.countplot(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31536,
     "status": "ok",
     "timestamp": 1559137442615,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "uo0GQJA1Drmy",
    "outputId": "906add85-8635-48c7-a7a1-acedd11a0240",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1415e7cf8>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(pitches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31371,
     "status": "ok",
     "timestamp": 1559137442616,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "kvY1xcgiDrm2",
    "outputId": "f23cb728-b5d2-4e26-a2c3-c4790f482376",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((900, 16, 121), (900, 2))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/val split\n",
    "n = 70000\n",
    "n = 1000\n",
    "n = min(n,X.shape[0])\n",
    "data = X[:n]\n",
    "labels = y[:n]\n",
    "\n",
    "perm = np.random.permutation(n)\n",
    "p = int(0.9*n)\n",
    "\n",
    "train_data = data[perm[:p]]\n",
    "train_labels = labels[perm[:p]]\n",
    "val_data = data[perm[p:]]\n",
    "val_labels = labels[perm[p:]]\n",
    "\n",
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b77owDQ-Drm5"
   },
   "source": [
    "### Load Data for Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert numpy ndarray to a Tensor (Unable to get element as bytes.).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-905baf47e363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   1440\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    296\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m   1875\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[1;32m   1876\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[0;32m-> 1877\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1878\u001b[0m       ])\n\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1875\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[1;32m   1876\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[0;32m-> 1877\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1878\u001b[0m       ])\n\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m   \"\"\"\n\u001b[0;32m-> 1039\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    302\u001b[0m                                          as_ref=False):\n\u001b[1;32m    303\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    243\u001b[0m   \"\"\"\n\u001b[1;32m    244\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 245\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert numpy ndarray to a Tensor (Unable to get element as bytes.)."
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ps1leCZDrm6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# weighted randomsampler!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IhxqWi8YDrm8"
   },
   "source": [
    "# C. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim_pitch, output_dim_duration, num_layers=1, dropout=0.3):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim_pitch = output_dim_pitch\n",
    "        self.output_dim_duration = output_dim_duration\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        #LSTM\n",
    "        if tf.test.is_gpu_available():\n",
    "            self.lstm = tf.keras.layers.CuDNNLSTM(self.hidden_dim, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            self.lstm = tf.keras.layers.LSTM(self.hidden_dim, \n",
    "                               return_sequences=False, \n",
    "                               return_state=False, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        \n",
    "        self.linear_pitch = tf.keras.layers.Dense(self.output_dim_pitch)\n",
    "        self.linear_duration = tf.keras.layers.Dense(self.output_dim_duration)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (tf.zeros((batch_size, self.hidden_dim)),\n",
    "                tf.zeros((batch_size, self.hidden_dim)))\n",
    "        \n",
    "    def call(self, x):\n",
    "        self.hidden = self.lstm(x, self.hidden)   \n",
    "        \n",
    "        fc_input = self.hidden\n",
    "        \n",
    "        pitch = self.linear_pitch(fc_input)\n",
    "        duration = self.linear_duration(fc_input)\n",
    "        return pitch, duration\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model_params = {\n",
    "    'input_dim': train_data.shape[-1],\n",
    "    'hidden_dim': 64,\n",
    "    'num_layers': 2,\n",
    "    'output_dim_pitch': MELODY,\n",
    "    'output_dim_duration': TIMES\n",
    "}\n",
    "\n",
    "lstm = LSTM(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=tf.losses.sparse_softmax_cross_entropy\n",
    "optimizer = tf.train.AdamOptimizer() \n",
    "train_acc_metric = tf.keras.metrics.sparse_categorical_accuracy\n",
    "val_acc_metric = tf.keras.metrics.sparse_categorical_accuracy\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 141.9242\n",
      "Time taken 4.3 sec\n",
      "\n",
      "Epoch:  0 / 10 in 4.31 s loss:  141.9242 acc_train:  0.2 acc_val:  0\n",
      "Epoch 2 Loss 70.4520\n",
      "Time taken 8.1 sec\n",
      "\n",
      "Epoch 3 Loss 62.9046\n",
      "Time taken 11.7 sec\n",
      "\n",
      "Epoch:  2 / 10 in 11.66 s loss:  62.9046 acc_train:  0.25 acc_val:  0\n",
      "Epoch 4 Loss 60.6735\n",
      "Time taken 15.3 sec\n",
      "\n",
      "Epoch 5 Loss 58.7203\n",
      "Time taken 20.0 sec\n",
      "\n",
      "Epoch:  4 / 10 in 19.95 s loss:  58.7203 acc_train:  0.34 acc_val:  0\n",
      "Epoch 6 Loss 56.4181\n",
      "Time taken 23.9 sec\n",
      "\n",
      "Epoch 7 Loss 53.3869\n",
      "Time taken 28.4 sec\n",
      "\n",
      "Epoch:  6 / 10 in 28.43 s loss:  53.3869 acc_train:  0.5 acc_val:  0\n",
      "Epoch 8 Loss 49.7654\n",
      "Time taken 32.3 sec\n",
      "\n",
      "Epoch 9 Loss 46.0098\n",
      "Time taken 36.1 sec\n",
      "\n",
      "Epoch:  8 / 10 in 36.08 s loss:  46.0098 acc_train:  0.57 acc_val:  0\n",
      "Epoch 10 Loss 42.4009\n",
      "Time taken 39.9 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "start = time.time()\n",
    "hist = np.zeros(EPOCHS)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    pacc = 0\n",
    "    total=0\n",
    "    correct = 0\n",
    "    \n",
    "    for (batch, (inputs, labels)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            batch_size = inputs.shape[0]\n",
    "            lstm.hidden = lstm.init_hidden(batch_size)\n",
    "\n",
    "            pitch, duration = lstm(inputs)\n",
    "\n",
    "            loss_pitch = loss_fn(labels[:,0], pitch)\n",
    "            loss_duration = loss_fn(labels[:,1], duration)\n",
    "\n",
    "            batch_loss = loss_pitch + loss_duration\n",
    "            \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "\n",
    "        #backprop\n",
    "        gradients = tape.gradient(batch_loss, lstm.variables)\n",
    "        optimizer.apply_gradients(zip(gradients, lstm.variables))\n",
    "        \n",
    "        #eval\n",
    "        predicted = tf.argmax(pitch, 1)\n",
    "        total += int(labels[:,0].shape[0])\n",
    "        correct += sum(predicted.numpy()==labels[:,0].numpy())\n",
    "        \n",
    "    hist[epoch] = total_loss\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    if epoch%2==0:\n",
    "        notes = epoch%50==0\n",
    "        acc_train = correct/total\n",
    "        acc_val = 0\n",
    "        #acc_val, dacc_val, hacc_val = evaluation(model, val_loader, notes=False)\n",
    "        print('Epoch: ',epoch,'/',EPOCHS,'in', np.round(time.time()-start,2),'s',\\\n",
    "              \"loss: \", np.round(hist[epoch], 4), \"acc_train: \", np.round(acc_train,2), \"acc_val: \", np.round(acc_val,2))\n",
    "        #print(\"loss: \", np.round(hist[epoch], 4), \"dacc_train: \", np.round(dacc_train,2), \"dacc_val: \", np.round(dacc_val,2), \\\n",
    "        #\"hacc_train: \", np.round(hacc_train,2), \"hacc_val: \", np.round(hacc_val,2))\n",
    "\n",
    "        if acc_train >= 90:\n",
    "            print(\"finishing training at acc_train > 90\")\n",
    "            break\n",
    "plt.plot(hist, label=\"Training loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    hist = np.zeros(num_epochs)\n",
    "\n",
    "    def clip_gradient(model, clip_value):\n",
    "        params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "        for p in params:\n",
    "            p.grad.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "    starttime = time.time()\n",
    "    for t in range(num_epochs):\n",
    "        loss_curr = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "            model.hidden = model.init_hidden(batch_size)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            pitch, duration = model(inputs.float())\n",
    "\n",
    "            loss_pitch = loss_fn(pitch, labels[:,0].long())\n",
    "            loss_duration = loss_fn(duration, labels[:,1].long())\n",
    "\n",
    "            loss = loss_pitch + loss_duration\n",
    "\n",
    "            loss.backward()\n",
    "            clip_gradient(model, 1)\n",
    "            optim.step()\n",
    "\n",
    "            loss_curr = loss_curr + loss.item()\n",
    "\n",
    "        hist[t] = loss_curr\n",
    "        if t%10==0:\n",
    "            notes = t%50==0\n",
    "            acc_train, dacc_train, hacc_train = evaluation(model, train_loader, notes)\n",
    "            acc_val, dacc_val, hacc_val = evaluation(model, val_loader, notes=False)\n",
    "            print('Epoch: ',t,'/',num_epochs,'in', np.round(time.time()-starttime,2),'s',\\\n",
    "                  \"loss: \", np.round(hist[t], 4), \"acc_train: \", np.round(acc_train,2), \"acc_val: \", np.round(acc_val,2))\n",
    "            print(\"loss: \", np.round(hist[t], 4), \"dacc_train: \", np.round(dacc_train,2), \"dacc_val: \", np.round(dacc_val,2), \\\n",
    "            \"hacc_train: \", np.round(hacc_train,2), \"hacc_val: \", np.round(hacc_val,2))\n",
    "        \n",
    "        if acc_train >= 90:\n",
    "            print(\"finishing training at acc_train > 90\")\n",
    "            break\n",
    "    plt.plot(hist, label=\"Training loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b65bf17f20fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2288\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2291\u001b[0m       \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2529\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-aaf52d12be73>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mfc_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m       raise TypeError(\n\u001b[0;32m--> 442\u001b[0;31m           \u001b[0;34m\"Tensor objects are only iterable when eager execution is \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m           \"enabled. To iterate over this tensor use tf.map_fn.\")\n\u001b[1;32m    444\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn."
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = lstm(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    logits = model(images, training=True)\n",
    "    loss_value = loss(logits, labels)\n",
    "grads = tape.gradient(loss_value, model.variables)\n",
    "optimizer.apply_gradients(zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training(Optimizer)\n",
    "train_params = {\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'optim': tf.train.AdamOptimizer(0.001),\n",
    "    'loss_fn': torch.nn.CrossEntropyLoss(),\n",
    "    'num_epochs': 100\n",
    "}\n",
    "\n",
    "train(model, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features, labels_placeholder: labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-Yq05qsDrm9"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# Build model\n",
    "#####################\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim_pitch, output_dim_duration, num_layers=1, dropout=0.5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim_pitch = output_dim_pitch\n",
    "        self.output_dim_duration = output_dim_duration\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        \n",
    "        self.linear_pitch = nn.Linear(self.hidden_dim, self.output_dim_pitch)\n",
    "        self.linear_duration = nn.Linear(self.hidden_dim, self.output_dim_duration)\n",
    "        \n",
    "        \n",
    "        print(\"Nr. of params:\", self.get_nr_params())\n",
    "    \n",
    "    def get_nr_params(self):\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        return params\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(x.permute(1,0,2), self.hidden)\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        fc_input = lstm_out[-1,:,:].contiguous()\n",
    "        #fc_input = self.bn2(lstm_out[:,-1,:])\n",
    "\n",
    "        \n",
    "        pitch = self.linear_pitch(fc_input)\n",
    "        duration = self.linear_duration(fc_input)\n",
    "        # log_probs = F.log_softmax(y_pred,dim=1) # crossentropy is already with softmax\n",
    "        return pitch, duration\n",
    "\n",
    "def train(model, train_loader, val_loader, optim, loss_fn, num_epochs=100):\n",
    "\n",
    "    hist = np.zeros(num_epochs)\n",
    "\n",
    "    def clip_gradient(model, clip_value):\n",
    "        params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "        for p in params:\n",
    "            p.grad.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "    starttime = time.time()\n",
    "    for t in range(num_epochs):\n",
    "        loss_curr = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "            model.hidden = model.init_hidden(batch_size)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            pitch, duration = model(inputs.float())\n",
    "\n",
    "            loss_pitch = loss_fn(pitch, labels[:,0].long())\n",
    "            loss_duration = loss_fn(duration, labels[:,1].long())\n",
    "\n",
    "            loss = loss_pitch + loss_duration\n",
    "\n",
    "            loss.backward()\n",
    "            clip_gradient(model, 1)\n",
    "            optim.step()\n",
    "\n",
    "            loss_curr = loss_curr + loss.item()\n",
    "\n",
    "        hist[t] = loss_curr\n",
    "        if t%10==0:\n",
    "            notes = t%50==0\n",
    "            acc_train, dacc_train, hacc_train = evaluation(model, train_loader, notes)\n",
    "            acc_val, dacc_val, hacc_val = evaluation(model, val_loader, notes=False)\n",
    "            print('Epoch: ',t,'/',num_epochs,'in', np.round(time.time()-starttime,2),'s',\\\n",
    "                  \"loss: \", np.round(hist[t], 4), \"acc_train: \", np.round(acc_train,2), \"acc_val: \", np.round(acc_val,2))\n",
    "            print(\"loss: \", np.round(hist[t], 4), \"dacc_train: \", np.round(dacc_train,2), \"dacc_val: \", np.round(dacc_val,2), \\\n",
    "            \"hacc_train: \", np.round(hacc_train,2), \"hacc_val: \", np.round(hacc_val,2))\n",
    "        \n",
    "        if acc_train >= 90:\n",
    "            print(\"finishing training at acc_train > 90\")\n",
    "            break\n",
    "    plt.plot(hist, label=\"Training loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def evaluation(model, loader, notes=False):\n",
    "    model.eval()\n",
    "    correct, total, dcorrect, dtotal = 0,0,0,0 \n",
    "    hcorrect, htotal = 0,0\n",
    "    for i, (inputs, labels) in enumerate(loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        model.hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        pitch, duration = model(inputs.float())\n",
    "        \n",
    "        _, predicted = torch.max(pitch.data, 1)\n",
    "        total += labels[:,0].size(0) \n",
    "        correct += (predicted == labels[:,0].long()).sum().item()\n",
    "        \n",
    "        _, dpredicted = torch.max(duration.data, 1)\n",
    "        dtotal += labels[:,1].size(0) \n",
    "        dcorrect += (dpredicted == labels[:,1].long()).sum().item()\n",
    "        \n",
    "        # Indicator: check if pitch are in harmony of currchord OR nextchord\n",
    "        \"\"\"\n",
    "        predicted = predicted.cpu()\n",
    "        batches = np.where(predicted < MELODY-1)[0]\n",
    "        checker = predicted[batches]\n",
    "        check = inputs[-1,batches,inputs.shape[-1]-24:-12] #only works if melodyEncoding configured accordingly\n",
    "        check2 = inputs[-1,batches,inputs.shape[-1]-12:] # dim (batches,12)\n",
    "        check = check+check2\n",
    "        for i in range(batches.shape[0]):\n",
    "            if check[i,checker[i] % 12] > 1:\n",
    "                hcorrect += 1 \n",
    "            htotal += 1\n",
    "            \"\"\"\n",
    "            \n",
    "        \n",
    "        if notes and i == 0:\n",
    "            print(predicted)\n",
    "            print(labels[:,0])\n",
    "            \n",
    "    \n",
    "    acc = int(100 * correct / total)\n",
    "    dacc = int(100 * dcorrect / dtotal)\n",
    "    hacc = 0 # int(100 * hcorrect / htotal)\n",
    "    model.train()\n",
    "    return acc, dacc, hacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TFFF5j5ADrm_"
   },
   "source": [
    "### Training: we need 3 things: \n",
    "- Data(loader), \n",
    "- Model, \n",
    "- Optimization(Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1559138293189,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "74kfvDaCDrnA",
    "outputId": "7e68935c-3bb3-41d7-c805-a80c3131d5a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of params: 86805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(121, 64, num_layers=2, dropout=0.5)\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear_pitch): Linear(in_features=64, out_features=37, bias=True)\n",
       "  (linear_duration): Linear(in_features=64, out_features=48, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data(loader)\n",
    "batch_size = 32\n",
    "train_loader = getDataLoader(train_data, train_labels, batch_size, sample=True)\n",
    "val_loader = getDataLoader(train_data, train_labels, batch_size, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model_params = {\n",
    "    'input_dim': train_data.shape[-1],\n",
    "    'hidden_dim': 64,\n",
    "    'num_layers': 2,\n",
    "    'output_dim_pitch': MELODY,\n",
    "    'output_dim_duration': TIMES\n",
    "}\n",
    "\n",
    "model = LSTM(**model_params)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 813
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45131,
     "status": "ok",
     "timestamp": 1559138338038,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "A8waYTcPDrnG",
    "outputId": "5f5b4d95-097c-4cfe-ac47-deac03352ba9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8])\n",
      "tensor([ 1,  3, 11,  3,  6,  8,  8,  8,  4,  1,  4,  1,  3,  8,  8,  8,  1,  8,\n",
      "         8,  1,  6,  7,  6,  8,  1,  3,  8, 11,  4,  8,  8,  8])\n",
      "Epoch:  0 / 100 in 1.69 s loss:  177.6351 acc_train:  44 acc_val:  18\n",
      "loss:  177.6351 dacc_train:  31 dacc_val:  6 hacc_train:  0 hacc_val:  0\n",
      "Epoch:  10 / 100 in 12.08 s loss:  52.7901 acc_train:  49 acc_val:  23\n",
      "loss:  52.7901 dacc_train:  73 dacc_val:  90 hacc_train:  0 hacc_val:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-84e0d3f560f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-9bde82b50e63>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optim, loss_fn, num_epochs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_pitch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_duration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mclip_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training(Optimizer)\n",
    "train_params = {\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': val_loader,\n",
    "    'optim': torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "    #optim: torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9),\n",
    "    'loss_fn': torch.nn.CrossEntropyLoss(),\n",
    "    'num_epochs': 100\n",
    "}\n",
    "\n",
    "train(model, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jAj_Z8VpDrnK"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'weights/model'+datetime.datetime.now().strftime(\"%Y-%m-%d%H:%M:%S\")+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zLWgx6pqDrnO"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'weights/modelbig1'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1559131622854,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "UEP8HZRADrnR",
    "outputId": "95dccec1-ca08-4f4f-910d-2c50352b7a89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('weights/modelbig1'+'.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8318,
     "status": "ok",
     "timestamp": 1559131719214,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "-IojGngefcl1",
    "outputId": "6eef2c31-edc2-497d-98d3-62ed8d550038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/50/e4a5a869093f35884d1fd95b46b24705ab27adb7e562a2a307523c043be3/onnx-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
      "\u001b[K     || 7.1MB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx) (1.12.0)\n",
      "Requirement already satisfied: typing>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from onnx) (3.6.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx) (1.16.3)\n",
      "Collecting typing-extensions>=3.6.2.1 (from onnx)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/62/c66e553258c37c33f9939abb2dd8d2481803d860ff68e635466f12aa7efa/typing_extensions-3.7.2-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx) (3.7.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx) (41.0.1)\n",
      "Installing collected packages: typing-extensions, onnx\n",
      "Successfully installed onnx-1.5.0 typing-extensions-3.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAzFb-lIe7x5"
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "import torch.onnx\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    model.hidden = model.init_hidden(1)\n",
    "\n",
    "    dummy_input = torch.randn(np.expand_dims(train_data[0],0).shape).to(device)\n",
    "\n",
    "    torch.onnx.export(model, dummy_input, 'weights/modelbig1.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1853
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1559138828966,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "FWq8Svg4e8iA",
    "outputId": "147ee947-cef0-4718-f34e-f52fbd09d313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch-jit-export (\n",
      "  %0[FLOAT, 1x16x121]\n",
      ") initializers (\n",
      "  %1[FLOAT, 256x121]\n",
      "  %2[FLOAT, 256x64]\n",
      "  %3[FLOAT, 256]\n",
      "  %4[FLOAT, 256]\n",
      "  %5[FLOAT, 256x64]\n",
      "  %6[FLOAT, 256x64]\n",
      "  %7[FLOAT, 256]\n",
      "  %8[FLOAT, 256]\n",
      "  %9[FLOAT, 64]\n",
      "  %10[FLOAT, 64]\n",
      "  %11[FLOAT, 64]\n",
      "  %12[FLOAT, 64]\n",
      "  %13[INT64, scalar]\n",
      "  %14[FLOAT, 37x64]\n",
      "  %15[FLOAT, 37]\n",
      "  %16[FLOAT, 48x64]\n",
      "  %17[FLOAT, 48]\n",
      ") {\n",
      "  %18 = Transpose[perm = [1, 0, 2]](%0)\n",
      "  %20 = Slice[axes = [0], ends = [64], starts = [0]](%1)\n",
      "  %21 = Slice[axes = [0], ends = [256], starts = [192]](%1)\n",
      "  %22 = Slice[axes = [0], ends = [192], starts = [64]](%1)\n",
      "  %23 = Concat[axis = 0](%20, %21, %22)\n",
      "  %24 = Slice[axes = [0], ends = [64], starts = [0]](%2)\n",
      "  %25 = Slice[axes = [0], ends = [256], starts = [192]](%2)\n",
      "  %26 = Slice[axes = [0], ends = [192], starts = [64]](%2)\n",
      "  %27 = Concat[axis = 0](%24, %25, %26)\n",
      "  %28 = Slice[axes = [0], ends = [64], starts = [0]](%3)\n",
      "  %29 = Slice[axes = [0], ends = [256], starts = [192]](%3)\n",
      "  %30 = Slice[axes = [0], ends = [192], starts = [64]](%3)\n",
      "  %31 = Concat[axis = 0](%28, %29, %30)\n",
      "  %32 = Slice[axes = [0], ends = [64], starts = [0]](%4)\n",
      "  %33 = Slice[axes = [0], ends = [256], starts = [192]](%4)\n",
      "  %34 = Slice[axes = [0], ends = [192], starts = [64]](%4)\n",
      "  %35 = Concat[axis = 0](%32, %33, %34)\n",
      "  %36 = Concat[axis = 0](%31, %35)\n",
      "  %37 = Unsqueeze[axes = [0]](%23)\n",
      "  %38 = Unsqueeze[axes = [0]](%27)\n",
      "  %39 = Unsqueeze[axes = [0]](%36)\n",
      "  %40 = Shape(%18)\n",
      "  %41 = Constant[value = <Scalar Tensor []>]()\n",
      "  %42 = Gather(%40, %41)\n",
      "  %43 = Unsqueeze[axes = [0]](%42)\n",
      "  %44 = Constant[value = <Tensor>]()\n",
      "  %45 = Constant[value = <Scalar Tensor []>]()\n",
      "  %46 = Unsqueeze[axes = [0]](%45)\n",
      "  %47 = Concat[axis = 0](%46, %43, %44)\n",
      "  %48 = ConstantFill[input_as_shape = 1](%47)\n",
      "  %49 = Shape(%18)\n",
      "  %50 = Constant[value = <Scalar Tensor []>]()\n",
      "  %51 = Gather(%49, %50)\n",
      "  %52 = Unsqueeze[axes = [0]](%51)\n",
      "  %53 = Constant[value = <Tensor>]()\n",
      "  %54 = Constant[value = <Scalar Tensor []>]()\n",
      "  %55 = Unsqueeze[axes = [0]](%54)\n",
      "  %56 = Concat[axis = 0](%55, %52, %53)\n",
      "  %57 = ConstantFill[input_as_shape = 1](%56)\n",
      "  %58, %59, %60 = LSTM[hidden_size = 64](%18, %37, %38, %39, %, %48, %57)\n",
      "  %61 = Squeeze[axes = [1]](%58)\n",
      "  %62 = Slice[axes = [0], ends = [64], starts = [0]](%5)\n",
      "  %63 = Slice[axes = [0], ends = [256], starts = [192]](%5)\n",
      "  %64 = Slice[axes = [0], ends = [192], starts = [64]](%5)\n",
      "  %65 = Concat[axis = 0](%62, %63, %64)\n",
      "  %66 = Slice[axes = [0], ends = [64], starts = [0]](%6)\n",
      "  %67 = Slice[axes = [0], ends = [256], starts = [192]](%6)\n",
      "  %68 = Slice[axes = [0], ends = [192], starts = [64]](%6)\n",
      "  %69 = Concat[axis = 0](%66, %67, %68)\n",
      "  %70 = Slice[axes = [0], ends = [64], starts = [0]](%7)\n",
      "  %71 = Slice[axes = [0], ends = [256], starts = [192]](%7)\n",
      "  %72 = Slice[axes = [0], ends = [192], starts = [64]](%7)\n",
      "  %73 = Concat[axis = 0](%70, %71, %72)\n",
      "  %74 = Slice[axes = [0], ends = [64], starts = [0]](%8)\n",
      "  %75 = Slice[axes = [0], ends = [256], starts = [192]](%8)\n",
      "  %76 = Slice[axes = [0], ends = [192], starts = [64]](%8)\n",
      "  %77 = Concat[axis = 0](%74, %75, %76)\n",
      "  %78 = Concat[axis = 0](%73, %77)\n",
      "  %79 = Unsqueeze[axes = [0]](%65)\n",
      "  %80 = Unsqueeze[axes = [0]](%69)\n",
      "  %81 = Unsqueeze[axes = [0]](%78)\n",
      "  %82 = Shape(%61)\n",
      "  %83 = Constant[value = <Scalar Tensor []>]()\n",
      "  %84 = Gather(%82, %83)\n",
      "  %85 = Unsqueeze[axes = [0]](%84)\n",
      "  %86 = Constant[value = <Tensor>]()\n",
      "  %87 = Constant[value = <Scalar Tensor []>]()\n",
      "  %88 = Unsqueeze[axes = [0]](%87)\n",
      "  %89 = Concat[axis = 0](%88, %85, %86)\n",
      "  %90 = ConstantFill[input_as_shape = 1](%89)\n",
      "  %91 = Shape(%61)\n",
      "  %92 = Constant[value = <Scalar Tensor []>]()\n",
      "  %93 = Gather(%91, %92)\n",
      "  %94 = Unsqueeze[axes = [0]](%93)\n",
      "  %95 = Constant[value = <Tensor>]()\n",
      "  %96 = Constant[value = <Scalar Tensor []>]()\n",
      "  %97 = Unsqueeze[axes = [0]](%96)\n",
      "  %98 = Concat[axis = 0](%97, %94, %95)\n",
      "  %99 = ConstantFill[input_as_shape = 1](%98)\n",
      "  %100, %101, %102 = LSTM[hidden_size = 64](%61, %79, %80, %81, %, %90, %99)\n",
      "  %103 = Squeeze[axes = [1]](%100)\n",
      "  %104 = Constant[value = <Scalar Tensor []>]()\n",
      "  %105 = Gather[axis = 0](%103, %104)\n",
      "  %106 = Slice[axes = [0], ends = [9223372036854775807], starts = [0]](%105)\n",
      "  %107 = Slice[axes = [1], ends = [9223372036854775807], starts = [0]](%106)\n",
      "  %108 = Gemm[alpha = 1, beta = 1, transB = 1](%107, %14, %15)\n",
      "  %109 = Gemm[alpha = 1, beta = 1, transB = 1](%107, %16, %17)\n",
      "  return %108, %109\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "modelload = onnx.load(\"weights/modelbig1.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(modelload)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(modelload.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iyK7IZAXDrnX"
   },
   "source": [
    "# Test Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56ENduBtDrnX"
   },
   "outputs": [],
   "source": [
    "# read test midi file ignore labels and get output from network\n",
    "#create midi file with input midi and prediction from network\n",
    "# simply read midi here to see how it sounds + plot\n",
    "\n",
    "def writeMidi(notes, times, file, originalfile, tempo=120, resolution=12):\n",
    "    pm = pretty_midi.PrettyMIDI(resolution=resolution, initial_tempo=tempo)\n",
    "    pminst = pretty_midi.Instrument(program=0)\n",
    "    currtime = 0\n",
    "    for i, note in enumerate(notes):\n",
    "        if note != MELODY-1:\n",
    "            pitch = note+LOW\n",
    "            start = pm.tick_to_time(int(currtime))\n",
    "            end = pm.tick_to_time(int(currtime+times[i]))\n",
    "            if end > start:\n",
    "                pmnote = pretty_midi.Note(velocity=100, pitch = pitch, start=start, end=end)\n",
    "                pminst.notes.append(pmnote)\n",
    "        currtime += times[i]+1\n",
    "    pm.instruments.append(pminst)\n",
    "    \n",
    "    #get original chords from file\n",
    "    ogpm = pretty_midi.PrettyMIDI(originalfile)\n",
    "    originalchords = pretty_midi.Instrument(program=0)\n",
    "    originalnotes = getMelodyAndHarmony(originalfile)['harmony']\n",
    "    for i, note in enumerate(originalnotes):\n",
    "        old = ogpm.time_to_tick(note.start)\n",
    "        start = pm.tick_to_time(int(ogpm.time_to_tick(note.start)/ogpm.resolution*pm.resolution))\n",
    "        end = pm.tick_to_time(int(ogpm.time_to_tick(note.end)/ogpm.resolution*pm.resolution))\n",
    "        originalchords.notes.append(pretty_midi.Note(velocity=50, pitch=note.pitch, start=start, end=end))\n",
    "    pm.instruments.append(originalchords)\n",
    "\n",
    "    pm.write(file)\n",
    "    return pm\n",
    "\n",
    "\n",
    "# legacy\n",
    "def getPredictedNotes(notes, times, chords):\n",
    "    test_data, test_labels = getInputSequences(notes,times,chords,encodingDict)\n",
    "    \n",
    "    test_loader = getDataLoader(test_data,test_labels, batch_size=32, shuffle=False)\n",
    "    \n",
    "    predicted_notes = np.empty(0)\n",
    "    predicted_durations = np.empty(0)\n",
    "    model.eval()\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        model.hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        pitch, duration = model(inputs.float())\n",
    "\n",
    "        _, pred_pitch = torch.max(pitch.data, 1)\n",
    "        _, pred_duration = torch.max(duration.data, 1)\n",
    "        predicted_notes = np.concatenate((predicted_notes,pred_pitch.detach().numpy()))\n",
    "        predicted_durations = np.concatenate((predicted_durations, pred_duration.detach().numpy()))\n",
    "    return predicted_notes, predicted_durations\n",
    "\n",
    "def plot_piano_roll(pm, start_pitch, end_pitch, fs=100):\n",
    "    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n",
    "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
    "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
    "     \n",
    "    \n",
    "def tradeFour(notes, times, chords, chordsByBeat):\n",
    "    predicted_notes, predicted_durations = [], []\n",
    "    mel_len=16+1 # wegen nextChord\n",
    "\n",
    "    notesnew = np.array(notes[-mel_len:])\n",
    "    timesnew = np.array(times[-mel_len:])\n",
    "    chordsnew = np.array(chords[-mel_len:])\n",
    "\n",
    "    currentbeat = 0\n",
    "    currenttime = 0\n",
    "    while sum(predicted_durations) < 4*TIMES:\n",
    "        feat = getFeatureVectors(notesnew, timesnew, chordsnew, encodingDict)\n",
    "        feat = torch.Tensor(feat).unsqueeze(0).to(device)\n",
    "\n",
    "        model.hidden = model.init_hidden(feat.size(0))\n",
    "        pitch, duration = model(feat.float())\n",
    "\n",
    "        pred_pitch = torch.max(pitch.data, 1)[1].cpu().detach().numpy()[0]\n",
    "        pred_duration = torch.max(duration.data, 1)[1].cpu().detach().numpy()[0]\n",
    "\n",
    "        predicted_notes.append(pred_pitch)\n",
    "        predicted_durations.append(pred_duration)\n",
    "        notesnew = np.append(notesnew[1:], pred_pitch)\n",
    "        timesnew = np.append(timesnew[1:], pred_duration)\n",
    "        \n",
    "        if int(currenttime/12) > currentbeat:\n",
    "            newchord = chordsByBeat[int(currenttime/12)]\n",
    "            chordsnew = np.append(chordsnew[1:], newchord)\n",
    "        currenttime += pred_duration\n",
    "        currentbeat += int(currenttime/12)\n",
    "        \n",
    "        print(predicted_durations)\n",
    "        \n",
    "    notes = np.append(notes, np.array(predicted_notes))\n",
    "    times = np.append(times, np.array(predicted_durations))\n",
    "\n",
    "    return notes, times\n",
    "    \n",
    "def plotTradeFour(file):\n",
    "    notes, times, chords, chordsByBeat = processMidi(file)\n",
    "    \n",
    "    t = time.time()\n",
    "    notes, times = tradeFour(notes, times, chords, chordsByBeat)\n",
    "    print(time.time()-t,'s')\n",
    "    \n",
    "    # plotWithAudio\n",
    "    pm = writeMidi(notes, times, 'treval/newtest.mid', file)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plot_piano_roll(pm, LOW, HIGH)\n",
    "    display(IPython.display.Audio(pm.synthesize(fs=44000), rate=44000))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596,
     "output_embedded_package_id": "1eCs_57xjw6eJqmDAy4whOvMhN_oF7gC7"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3582,
     "status": "ok",
     "timestamp": 1558979993931,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "jitsFsWbDrna",
    "outputId": "5f452100-38d5-49d8-83e3-b67aa01c25e0",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTradeFour('test/testautumnleaves7.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800,
     "output_embedded_package_id": "1rjyqe0XYd0zf0ukdHUT-KmgqjTW2pVos"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3665,
     "status": "ok",
     "timestamp": 1558980017183,
     "user": {
      "displayName": "Paddy Wilson",
      "photoUrl": "https://lh3.googleusercontent.com/-Lq7fKfmUlNg/AAAAAAAAAAI/AAAAAAAACRI/cFf3IdBdoQA/s64/photo.jpg",
      "userId": "00502307669733282628"
     },
     "user_tz": -120
    },
    "id": "67HGJoCRDrne",
    "outputId": "928d3608-26b0-4122-9fa5-ec77e17289b2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotTradeFour('test/test12bar1.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1z8g4E6UDrnj"
   },
   "source": [
    "## Test A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_NTDPAtDrnk"
   },
   "outputs": [],
   "source": [
    "# test how much % model can learn a melody\n",
    "\n",
    "# define test melody\n",
    "# train network\n",
    "# give percentage of how much it learnt\n",
    "\n",
    "chordsDict={\n",
    "    'C': [1,0,0,0,1,0,0,1,0,0,0,0],\n",
    "    'C7': [1,0,0,0,1,0,0,1,0,0,1,0],\n",
    "    'Cmaj7': [1,0,0,0,1,0,0,1,0,0,0,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3bpvPeiDrnn",
    "outputId": "220e7428-964b-4cfc-cc61-7a60241946f8",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 16, 31)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of params: 30485\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "notes = [0,2,4,7,7,4,2,0,0,2,4,7,7,4,2,0,0,5,3,4,5,8,7,5,0,2,4,7,7,4,2,0]\n",
    "times = np.ones(32)*8\n",
    "chord = chordsDict['C']\n",
    "chords = np.repeat(chord, 32).reshape((12,-1)).T\n",
    "encoding = True\n",
    "train_data, train_labels = getInputSequences(notes, times, chords, encoding=encoding, modulation=False, padding=False, seq_len=1)\n",
    "\n",
    "display(train_data.shape)\n",
    "batch_size = 32\n",
    "train_loader = getDataLoader(train_data, train_labels, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# test model\n",
    "model_params = {\n",
    "    'input_dim': train_data.shape[-1],\n",
    "    'hidden_dim': 64,\n",
    "    'num_layers': 1,\n",
    "    'output_dim_pitch': MELODY,\n",
    "    'output_dim_duration': TIMES,\n",
    "    'dropout': 0\n",
    "}\n",
    "\n",
    "testmodel = LSTM(**model_params)\n",
    "testmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gelE-IxbDrnq",
    "outputId": "714c3e78-b332-46d7-b07c-704a2c735345",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  7.3719 acc_train:  6 acc_val:  6\n",
      "loss:  1.8806 acc_train:  20 acc_val:  20\n",
      "loss:  1.8009 acc_train:  26 acc_val:  26\n",
      "loss:  1.6949 acc_train:  33 acc_val:  33\n",
      "loss:  1.5203 acc_train:  53 acc_val:  53\n",
      "tensor([0, 5, 8, 4, 5, 8, 7, 5, 0, 7, 4, 7, 5, 4, 7])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  1.2851 acc_train:  73 acc_val:  73\n",
      "loss:  1.0255 acc_train:  80 acc_val:  80\n",
      "loss:  0.8072 acc_train:  86 acc_val:  86\n",
      "loss:  0.6302 acc_train:  86 acc_val:  86\n",
      "loss:  0.4656 acc_train:  93 acc_val:  93\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.3227 acc_train:  100 acc_val:  100\n",
      "loss:  0.2176 acc_train:  100 acc_val:  100\n",
      "loss:  0.1449 acc_train:  100 acc_val:  100\n",
      "loss:  0.1 acc_train:  100 acc_val:  100\n",
      "loss:  0.0731 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0561 acc_train:  100 acc_val:  100\n",
      "loss:  0.0446 acc_train:  100 acc_val:  100\n",
      "loss:  0.0366 acc_train:  100 acc_val:  100\n",
      "loss:  0.0308 acc_train:  100 acc_val:  100\n",
      "loss:  0.0264 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0231 acc_train:  100 acc_val:  100\n",
      "loss:  0.0204 acc_train:  100 acc_val:  100\n",
      "loss:  0.0182 acc_train:  100 acc_val:  100\n",
      "loss:  0.0164 acc_train:  100 acc_val:  100\n",
      "loss:  0.0149 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0136 acc_train:  100 acc_val:  100\n",
      "loss:  0.0125 acc_train:  100 acc_val:  100\n",
      "loss:  0.0116 acc_train:  100 acc_val:  100\n",
      "loss:  0.0107 acc_train:  100 acc_val:  100\n",
      "loss:  0.01 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0093 acc_train:  100 acc_val:  100\n",
      "loss:  0.0087 acc_train:  100 acc_val:  100\n",
      "loss:  0.0082 acc_train:  100 acc_val:  100\n",
      "loss:  0.0077 acc_train:  100 acc_val:  100\n",
      "loss:  0.0073 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0069 acc_train:  100 acc_val:  100\n",
      "loss:  0.0066 acc_train:  100 acc_val:  100\n",
      "loss:  0.0062 acc_train:  100 acc_val:  100\n",
      "loss:  0.0059 acc_train:  100 acc_val:  100\n",
      "loss:  0.0056 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0054 acc_train:  100 acc_val:  100\n",
      "loss:  0.0052 acc_train:  100 acc_val:  100\n",
      "loss:  0.0049 acc_train:  100 acc_val:  100\n",
      "loss:  0.0047 acc_train:  100 acc_val:  100\n",
      "loss:  0.0045 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0044 acc_train:  100 acc_val:  100\n",
      "loss:  0.0042 acc_train:  100 acc_val:  100\n",
      "loss:  0.004 acc_train:  100 acc_val:  100\n",
      "loss:  0.0039 acc_train:  100 acc_val:  100\n",
      "loss:  0.0037 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0036 acc_train:  100 acc_val:  100\n",
      "loss:  0.0035 acc_train:  100 acc_val:  100\n",
      "loss:  0.0034 acc_train:  100 acc_val:  100\n",
      "loss:  0.0033 acc_train:  100 acc_val:  100\n",
      "loss:  0.0032 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0031 acc_train:  100 acc_val:  100\n",
      "loss:  0.003 acc_train:  100 acc_val:  100\n",
      "loss:  0.0029 acc_train:  100 acc_val:  100\n",
      "loss:  0.0028 acc_train:  100 acc_val:  100\n",
      "loss:  0.0027 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0026 acc_train:  100 acc_val:  100\n",
      "loss:  0.0025 acc_train:  100 acc_val:  100\n",
      "loss:  0.0025 acc_train:  100 acc_val:  100\n",
      "loss:  0.0024 acc_train:  100 acc_val:  100\n",
      "loss:  0.0023 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0023 acc_train:  100 acc_val:  100\n",
      "loss:  0.0022 acc_train:  100 acc_val:  100\n",
      "loss:  0.0022 acc_train:  100 acc_val:  100\n",
      "loss:  0.0021 acc_train:  100 acc_val:  100\n",
      "loss:  0.0021 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.002 acc_train:  100 acc_val:  100\n",
      "loss:  0.002 acc_train:  100 acc_val:  100\n",
      "loss:  0.0019 acc_train:  100 acc_val:  100\n",
      "loss:  0.0019 acc_train:  100 acc_val:  100\n",
      "loss:  0.0018 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0018 acc_train:  100 acc_val:  100\n",
      "loss:  0.0017 acc_train:  100 acc_val:  100\n",
      "loss:  0.0017 acc_train:  100 acc_val:  100\n",
      "loss:  0.0017 acc_train:  100 acc_val:  100\n",
      "loss:  0.0016 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0016 acc_train:  100 acc_val:  100\n",
      "loss:  0.0016 acc_train:  100 acc_val:  100\n",
      "loss:  0.0015 acc_train:  100 acc_val:  100\n",
      "loss:  0.0015 acc_train:  100 acc_val:  100\n",
      "loss:  0.0015 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0014 acc_train:  100 acc_val:  100\n",
      "loss:  0.0014 acc_train:  100 acc_val:  100\n",
      "loss:  0.0014 acc_train:  100 acc_val:  100\n",
      "loss:  0.0013 acc_train:  100 acc_val:  100\n",
      "loss:  0.0013 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0013 acc_train:  100 acc_val:  100\n",
      "loss:  0.0013 acc_train:  100 acc_val:  100\n",
      "loss:  0.0012 acc_train:  100 acc_val:  100\n",
      "loss:  0.0012 acc_train:  100 acc_val:  100\n",
      "loss:  0.0012 acc_train:  100 acc_val:  100\n",
      "tensor([0, 5, 3, 4, 5, 8, 7, 5, 0, 2, 4, 7, 7, 4, 2])\n",
      "tensor([0., 5., 3., 4., 5., 8., 7., 5., 0., 2., 4., 7., 7., 4., 2.],\n",
      "       dtype=torch.float64)\n",
      "loss:  0.0012 acc_train:  100 acc_val:  100\n",
      "loss:  0.0012 acc_train:  100 acc_val:  100\n",
      "loss:  0.0011 acc_train:  100 acc_val:  100\n",
      "loss:  0.0011 acc_train:  100 acc_val:  100\n",
      "loss:  0.0011 acc_train:  100 acc_val:  100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_params = {\n",
    "    'train_loader': train_loader,\n",
    "    'val_loader': train_loader,\n",
    "    'optim': torch.optim.Adam(testmodel.parameters(), lr=1e-2),\n",
    "    'loss_fn': torch.nn.CrossEntropyLoss(),\n",
    "    'num_epochs': 1000\n",
    "}\n",
    "train(testmodel, **train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ye3fEMEVDrnu"
   },
   "source": [
    "# Test B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXuGeoQKDrnv"
   },
   "outputs": [],
   "source": [
    "notes = np.ones(32)*8\n",
    "times = [8]\n",
    "chord = chordsDict['C']\n",
    "chords = np.repeat(chord, 32).reshape((12,-1)).T\n",
    "encoding = True\n",
    "train_data, train_labels = getInputSequences(notes, times, chords, encoding=encoding, modulation=False, padding=False, seq_len=1)\n",
    "\n",
    "display(train_data.shape)\n",
    "batch_size = 32\n",
    "train_loader = getDataLoader(train_data, train_labels, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# test model\n",
    "model_params = {\n",
    "    'input_dim': train_data.shape[-1],\n",
    "    'hidden_dim': 64,\n",
    "    'num_layers': 1,\n",
    "    'output_dim_pitch': MELODY,\n",
    "    'output_dim_duration': TIMES,\n",
    "    'dropout': 0\n",
    "}\n",
    "\n",
    "testmodel = LSTM(**model_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57ym3Kx-Drnz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zPqpYyNDrn3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIP8bzP9Drn6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gfDCQVeKDrn9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6frqMELbDroA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Amv8q3uDroC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EYwLvorgDroH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYXTGtgZDroJ"
   },
   "source": [
    "# Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsE3JcMZDroK",
    "outputId": "18dccb35-917b-4973-f19e-18885a76ddfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# smallest LSTM example\n",
    "rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True) # It was batch_first=True!\n",
    "inputs = torch.randn(5, 300, 10)\n",
    "hidden = (torch.randn(2, 5, 20), torch.randn(2, 5, 20))\n",
    "output, hidden = rnn(inputs, hidden)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLq1g6PHDroN"
   },
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    \"\"\" Move {0,1} data to {-1,1}, then standardize. Standardize data in [0,1]\"\"\"\n",
    "    data = data.astype(np.float32)\n",
    "    standardized_data = (data - np.mean(data, axis=0)[None,:,:]) / np.std(data, axis=0)[None,:,:]\n",
    "    return standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m4CGABNGDroP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JamBuddyDAC.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
